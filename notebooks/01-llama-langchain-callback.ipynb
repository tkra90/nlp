{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Introduction\n","The plan:\n","- load and use llama-cpp with langchain\n","- add custom CallbackHandler to track token usage"]},{"cell_type":"markdown","metadata":{},"source":["### Imports and installs"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T17:42:49.830750Z","iopub.status.busy":"2023-10-10T17:42:49.830359Z","iopub.status.idle":"2023-10-10T17:44:03.351878Z","shell.execute_reply":"2023-10-10T17:44:03.350782Z","shell.execute_reply.started":"2023-10-10T17:42:49.830717Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install -qqq langchain==0.0.304 --progress-bar off\n","%pip install -qqq llama-cpp-python==0.2.7 --progress-bar off"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T17:44:03.354064Z","iopub.status.busy":"2023-10-10T17:44:03.353807Z","iopub.status.idle":"2023-10-10T17:44:06.306111Z","shell.execute_reply":"2023-10-10T17:44:06.304325Z","shell.execute_reply.started":"2023-10-10T17:44:03.354041Z"},"trusted":true},"outputs":[],"source":["import os\n","import sys\n","import warnings\n","\n","warnings.simplefilter(\"ignore\")\n","\n","from huggingface_hub import hf_hub_download\n","from langchain.llms import LlamaCpp\n","from langchain import PromptTemplate, LLMChain\n","\n","from langchain.callbacks.manager import CallbackManager\n","from langchain.callbacks import get_openai_callback\n","from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler # handle std out of llm in jupyterNB"]},{"cell_type":"markdown","metadata":{},"source":["***\n","## Load Llama2-13b"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"execution":{"iopub.execute_input":"2023-10-10T17:44:06.308318Z","iopub.status.busy":"2023-10-10T17:44:06.307856Z","iopub.status.idle":"2023-10-10T17:44:37.887551Z","shell.execute_reply":"2023-10-10T17:44:37.886434Z","shell.execute_reply.started":"2023-10-10T17:44:06.308295Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGUF\"\n","model_basename = \"llama-2-13b-chat.Q4_K_M.gguf\"\n","model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n","\n","callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n","\n","n_gpu_layers = 32  # Change this value based on your model and your GPU VRAM pool.\n","n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n","\n","llm = LlamaCpp(\n","    model_path=model_path,\n","    max_tokens=2500,\n","    n_gpu_layers=n_gpu_layers,\n","    n_batch=n_batch,\n","    callback_manager=callback_manager,\n","    n_ctx=2500, # Context window\n","    verbose=True, # Verbose is required to pass to the callback manager\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Create prompt template and run chain"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T17:44:37.890144Z","iopub.status.busy":"2023-10-10T17:44:37.889304Z","iopub.status.idle":"2023-10-10T17:44:37.899467Z","shell.execute_reply":"2023-10-10T17:44:37.898072Z","shell.execute_reply.started":"2023-10-10T17:44:37.890098Z"},"trusted":true},"outputs":[],"source":["template = \"\"\"Question: {question}\n","\n","Answer: Answer briefly in a sentence!\"\"\"\n","\n","prompt = PromptTemplate(template=template, input_variables=[\"question\"])"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T17:44:37.904254Z","iopub.status.busy":"2023-10-10T17:44:37.903742Z","iopub.status.idle":"2023-10-10T17:47:12.460867Z","shell.execute_reply":"2023-10-10T17:47:12.457300Z","shell.execute_reply.started":"2023-10-10T17:44:37.904215Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[" Albert Einstein was born on March 14th (3/14), 1879."]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time = 20219.05 ms\n","llama_print_timings:      sample time =    20.39 ms /    24 runs   (    0.85 ms per token,  1177.22 tokens per second)\n","llama_print_timings: prompt eval time = 20218.95 ms /    25 tokens (  808.76 ms per token,     1.24 tokens per second)\n","llama_print_timings:        eval time = 133941.85 ms /    23 runs   ( 5823.56 ms per token,     0.17 tokens per second)\n","llama_print_timings:       total time = 154335.42 ms\n"]}],"source":["chain = LLMChain(prompt=prompt, llm=llm)\n","\n","with get_openai_callback() as cb:\n","    result = chain.run({'question': \"When was Einstein born? Give year, month day.\",})"]},{"cell_type":"markdown","metadata":{},"source":["## Custom CallbackHandler"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T17:47:12.472159Z","iopub.status.busy":"2023-10-10T17:47:12.471174Z","iopub.status.idle":"2023-10-10T17:47:12.503945Z","shell.execute_reply":"2023-10-10T17:47:12.501852Z","shell.execute_reply.started":"2023-10-10T17:47:12.472121Z"},"trusted":true},"outputs":[],"source":["from typing import Any, Dict, List\n","from langchain.callbacks.base import BaseCallbackHandler\n","from langchain.schema import AgentAction, AgentFinish, LLMResult\n","from langchain.pydantic_v1 import BaseModel\n","from langchain.schema.messages import BaseMessage\n","\n","\n","class CustomTokenCounter(BaseCallbackHandler):\n","    \"\"\"Callback Handler that tracks token info.\"\"\"\n","    \n","    llama_model: LlamaCpp = None\n","    total_tokens: int = 0\n","    prompt_tokens: int = 0\n","    completion_tokens: int = 0\n","    successful_requests: int = 0\n","    total_cost: float = 0.0\n","    llprompts: List[str] = []\n","    llres: List[str] = []\n","        \n","    def __repr__(self) -> str:\n","        return (\n","            f\"\\tTokens Used: {self.total_tokens}\\n\"\n","            f\"\\tPrompt Tokens: {self.prompt_tokens}\\n\"\n","            f\"\\tCompletion Tokens: {self.completion_tokens}\\n\"\n","        )\n","\n","    def on_llm_start(\n","        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n","    ) -> None:\n","        \"\"\"Run when LLM starts running.\"\"\"\n","        self.llprompts.append(prompts)\n","\n","    def on_chat_model_start(\n","        self,\n","        serialized: Dict[str, Any],\n","        messages: List[List[BaseMessage]],\n","        **kwargs: Any\n","    ) -> None:\n","        \"\"\"Run when LLM starts running.\"\"\"\n","\n","    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n","        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n","        self.total_tokens += 1\n","\n","    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n","        \"\"\"Run when LLM ends running.\"\"\"\n","        if response.llm_output is None:  \n","            return None\n","        else:\n","            text_responses = [gen.text for gens in response.generations for gen in gens]\n","            self.llres.append(text_responses)\n","\n","    def on_llm_error(self, error: BaseException, **kwargs: Any) -> None:\n","        \"\"\"Run when LLM errors.\"\"\"\n","\n","    def on_chain_start(\n","        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n","    ) -> None:\n","        \"\"\"Run when chain starts running.\"\"\"\n","        print(f'questions: {inputs[\"question\"]}')\n","        print(f\"on_chain_start {serialized['name']}\")\n","\n","    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:\n","        \"\"\"Run when chain ends running.\"\"\"\n","        print(f'Chain output: {outputs[\"text\"]}')\n","\n","    def on_chain_error(self, error: BaseException, **kwargs: Any) -> None:\n","        \"\"\"Run when chain errors.\"\"\"\n","\n","    def on_tool_start(\n","        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\n","    ) -> None:\n","        \"\"\"Run when tool starts running.\"\"\"\n","        print(f\"on_tool_start {serialized['name']}\")\n","\n","    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n","        \"\"\"Run on agent action.\"\"\"\n","        print(f\"on_agent_action {action}\")\n","\n","    def on_tool_end(self, output: str, **kwargs: Any) -> None:\n","        \"\"\"Run when tool ends running.\"\"\"\n","\n","    def on_tool_error(self, error: BaseException, **kwargs: Any) -> None:\n","        \"\"\"Run when tool errors.\"\"\"\n","\n","    def on_text(self, text: str, **kwargs: Any) -> None:\n","        \"\"\"Run on arbitrary text.\"\"\"\n","\n","    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> None:\n","        \"\"\"Run on agent end.\"\"\""]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T18:14:59.438981Z","iopub.status.busy":"2023-10-10T18:14:59.438406Z","iopub.status.idle":"2023-10-10T18:15:30.501301Z","shell.execute_reply":"2023-10-10T18:15:30.500185Z","shell.execute_reply.started":"2023-10-10T18:14:59.438950Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["questions: What is the Fermi paradox?\n"]},{"name":"stderr","output_type":"stream","text":["Llama.generate: prefix-match hit\n"]},{"name":"stdout","output_type":"stream","text":[" The Fermi Paradox is that, given the vast numbers of stars with planets and the probability that some have intelligent life, we should have seen evidence of extraterrestrial civilizations by now.output:  The Fermi Paradox is that, given the vast numbers of stars with planets and the probability that some have intelligent life, we should have seen evidence of extraterrestrial civilizations by now.\n"]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time = 20219.05 ms\n","llama_print_timings:      sample time =    28.78 ms /    45 runs   (    0.64 ms per token,  1563.59 tokens per second)\n","llama_print_timings: prompt eval time =  7734.64 ms /    20 tokens (  386.73 ms per token,     2.59 tokens per second)\n","llama_print_timings:        eval time = 23120.62 ms /    44 runs   (  525.47 ms per token,     1.90 tokens per second)\n","llama_print_timings:       total time = 31049.75 ms\n"]}],"source":["tkns = CustomTokenCounter()\n","chain = LLMChain(prompt=prompt, llm=llm, callbacks=[tkns])\n","\n","with get_openai_callback() as cb:\n","#     result = chain.run({'question': \"When was Einstein born? Give year, month day.\",})\n","    result = chain.run({'question': \"What is the Fermi paradox?\",})"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-10-10T18:33:54.221169Z","iopub.status.busy":"2023-10-10T18:33:54.220566Z","iopub.status.idle":"2023-10-10T18:33:54.233417Z","shell.execute_reply":"2023-10-10T18:33:54.231168Z","shell.execute_reply.started":"2023-10-10T18:33:54.221134Z"},"trusted":true},"outputs":[{"data":{"text/plain":["\tTokensss Used: 0\n","\tPrompt Tokens: 0\n","\tCompletion Tokens: 0"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["tkns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
