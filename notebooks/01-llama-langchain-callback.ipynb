{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Introduction\n","The plan:\n","- install, load and use llama-cpp with langchain\n","- add custom CallbackHandler to track token usage"]},{"cell_type":"markdown","metadata":{},"source":["### Imports and installs"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T12:12:13.023941Z","iopub.status.busy":"2023-10-11T12:12:13.023562Z","iopub.status.idle":"2023-10-11T12:12:13.063391Z","shell.execute_reply":"2023-10-11T12:12:13.062539Z","shell.execute_reply.started":"2023-10-11T12:12:13.023910Z"},"trusted":true},"outputs":[],"source":["import os\n","import sys\n","\n","import logging\n","import warnings\n","from importlib import reload\n","\n","warnings.simplefilter(\"ignore\")\n","\n","reload(logging)\n","logging.basicConfig(stream=sys.stdout, format='',\n","                    level=logging.INFO, datefmt=None)\n","log = logging.getLogger(__name__)\n","# log.info(\"This should print as normal output in Jupyter, and not in a red box\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T12:12:13.065871Z","iopub.status.busy":"2023-10-11T12:12:13.065110Z","iopub.status.idle":"2023-10-11T12:13:43.959808Z","shell.execute_reply":"2023-10-11T12:13:43.958302Z","shell.execute_reply.started":"2023-10-11T12:12:13.065841Z"},"trusted":true},"outputs":[],"source":["%%capture\n","%pip install -qqq langchain==0.0.304 --progress-bar off\n","%pip install -qqq llama-cpp-python==0.2.7 --progress-bar off"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T12:13:43.962364Z","iopub.status.busy":"2023-10-11T12:13:43.961874Z","iopub.status.idle":"2023-10-11T12:13:47.596517Z","shell.execute_reply":"2023-10-11T12:13:47.595452Z","shell.execute_reply.started":"2023-10-11T12:13:43.962325Z"},"trusted":true},"outputs":[],"source":["from huggingface_hub import hf_hub_download\n","from langchain.llms import LlamaCpp\n","from langchain import PromptTemplate, LLMChain\n","\n","from langchain.callbacks.manager import CallbackManager\n","from langchain.callbacks import get_openai_callback\n","from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler # handle std out of llm in jupyterNB"]},{"cell_type":"markdown","metadata":{},"source":["***\n","## Load Llama2-13b"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"execution":{"iopub.execute_input":"2023-10-11T12:13:47.599172Z","iopub.status.busy":"2023-10-11T12:13:47.598646Z","iopub.status.idle":"2023-10-11T12:14:29.756490Z","shell.execute_reply":"2023-10-11T12:14:29.755193Z","shell.execute_reply.started":"2023-10-11T12:13:47.599110Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGUF\"\n","model_basename = \"llama-2-13b-chat.Q4_K_M.gguf\"\n","model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n","\n","callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n","\n","n_gpu_layers = 32  # Change this value based on your model and your GPU VRAM pool.\n","n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n","\n","llm = LlamaCpp(\n","    model_path=model_path,\n","    max_tokens=2500,\n","    n_gpu_layers=n_gpu_layers,\n","    n_batch=n_batch,\n","    callback_manager=callback_manager,\n","    n_ctx=2500, # Context window\n","    verbose=True, # Verbose is required to pass to the callback manager\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Create prompt template and run chain"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T12:14:29.759055Z","iopub.status.busy":"2023-10-11T12:14:29.758184Z","iopub.status.idle":"2023-10-11T12:14:29.766906Z","shell.execute_reply":"2023-10-11T12:14:29.765429Z","shell.execute_reply.started":"2023-10-11T12:14:29.759006Z"},"trusted":true},"outputs":[],"source":["template = \"\"\"Question: {question}\n","\n","Answer: Answer briefly in a sentence!\"\"\"\n","\n","prompt = PromptTemplate(template=template, input_variables=[\"question\"])"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T12:14:29.768942Z","iopub.status.busy":"2023-10-11T12:14:29.768651Z","iopub.status.idle":"2023-10-11T12:15:06.024283Z","shell.execute_reply":"2023-10-11T12:15:06.023512Z","shell.execute_reply.started":"2023-10-11T12:14:29.768919Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Einstein was born on March 14, 1879."]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time = 20707.13 ms\n","llama_print_timings:      sample time =    13.37 ms /    20 runs   (    0.67 ms per token,  1496.00 tokens per second)\n","llama_print_timings: prompt eval time = 20707.02 ms /    25 tokens (  828.28 ms per token,     1.21 tokens per second)\n","llama_print_timings:        eval time = 14672.60 ms /    19 runs   (  772.24 ms per token,     1.29 tokens per second)\n","llama_print_timings:       total time = 35483.85 ms\n"]}],"source":["# %%capture --no-stdout\n","chain = LLMChain(prompt=prompt, llm=llm)\n","\n","with get_openai_callback() as cb:\n","    result = chain.run({'question': \"When was Einstein born? Give year, month day.\",})"]},{"cell_type":"markdown","metadata":{},"source":["## Custom CallbackHandler"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T12:15:06.026463Z","iopub.status.busy":"2023-10-11T12:15:06.025852Z","iopub.status.idle":"2023-10-11T12:15:06.044339Z","shell.execute_reply":"2023-10-11T12:15:06.043246Z","shell.execute_reply.started":"2023-10-11T12:15:06.026420Z"},"trusted":true},"outputs":[],"source":["from typing import Any, Dict, List\n","from langchain.callbacks.base import BaseCallbackHandler\n","from langchain.schema import AgentAction, AgentFinish, LLMResult\n","from langchain.pydantic_v1 import BaseModel\n","from langchain.schema.messages import BaseMessage\n","\n","\n","class CustomTokenCounter(BaseCallbackHandler):\n","    \"\"\"Callback Handler that tracks token info.\"\"\"\n","    \n","    llama_model: LlamaCpp = None\n","    total_tokens: int = 0\n","    prompt_tokens: int = 0\n","    completion_tokens: int = 0\n","    successful_requests: int = 0\n","    total_cost: float = 0.0\n","    llprompts: List[str] = []\n","    llres: List[str] = []\n","        \n","    def __repr__(self) -> str:\n","        return (\n","            f\"\\tTokens Used: {self.total_tokens}\\n\"\n","            f\"\\tPrompt Tokens: {self.prompt_tokens}\\n\"\n","            f\"\\tCompletion Tokens: {self.completion_tokens}\\n\"\n","        )\n","\n","    def on_llm_start(\n","        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n","    ) -> None:\n","        \"\"\"Run when LLM starts running.\"\"\"\n","        self.llprompts.append(prompts)\n","\n","    def on_chat_model_start(\n","        self,\n","        serialized: Dict[str, Any],\n","        messages: List[List[BaseMessage]],\n","        **kwargs: Any\n","    ) -> None:\n","        \"\"\"Run when LLM starts running.\"\"\"\n","\n","    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n","        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n","        # self.total_tokens += 1\n","\n","    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n","        \"\"\"Run when LLM ends running.\"\"\"\n","        if response.generations is None:  \n","            return None\n","        else:\n","            text_responses = [gen.text for gens in response.generations for gen in gens]\n","            self.llres.append(text_responses)\n","\n","    def on_llm_error(self, error: BaseException, **kwargs: Any) -> None:\n","        \"\"\"Run when LLM errors.\"\"\"\n","\n","    def on_chain_start(\n","        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n","    ) -> None:\n","        \"\"\"Run when chain starts running.\"\"\"\n","        print(f'questions: {inputs[\"question\"]}')\n","        print(f\"on_chain_start {serialized['name']}\")\n","\n","    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:\n","        \"\"\"Run when chain ends running.\"\"\"\n","        print(f'Chain output: {outputs[\"text\"]}')\n","\n","    def on_chain_error(self, error: BaseException, **kwargs: Any) -> None:\n","        \"\"\"Run when chain errors.\"\"\"\n","\n","    def on_tool_start(\n","        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\n","    ) -> None:\n","        \"\"\"Run when tool starts running.\"\"\"\n","        print(f\"on_tool_start {serialized['name']}\")\n","\n","    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n","        \"\"\"Run on agent action.\"\"\"\n","        print(f\"on_agent_action {action}\")\n","\n","    def on_tool_end(self, output: str, **kwargs: Any) -> None:\n","        \"\"\"Run when tool ends running.\"\"\"\n","\n","    def on_tool_error(self, error: BaseException, **kwargs: Any) -> None:\n","        \"\"\"Run when tool errors.\"\"\"\n","\n","    def on_text(self, text: str, **kwargs: Any) -> None:\n","        \"\"\"Run on arbitrary text.\"\"\"\n","\n","    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> None:\n","        \"\"\"Run on agent end.\"\"\"\n","        \n","    def count(self, llama_model: LlamaCpp)->str:\n","        self.prompt_tokens = sum(\n","            [llama_model.get_num_tokens(prompt[0]) for prompt in self.llprompts]\n","        )\n","        self.completion_tokens = sum(\n","            [llama_model.get_num_tokens(res[0]) for res in self.llres]\n","        )\n","        self.total_tokens = self.prompt_tokens + self.completion_tokens\n","        print (\n","            f\"\\tTokens Used: {self.total_tokens}\\n\"\n","            f\"\\tPrompt Tokens: {self.prompt_tokens}\\n\"\n","            f\"\\tCompletion Tokens: {self.completion_tokens}\\n\"\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2023-10-11T12:15:06.047164Z","iopub.status.busy":"2023-10-11T12:15:06.046068Z","iopub.status.idle":"2023-10-11T12:15:07.326093Z","shell.execute_reply":"2023-10-11T12:15:07.324986Z","shell.execute_reply.started":"2023-10-11T12:15:06.047100Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGUF\"\n","model_basename = \"llama-2-13b-chat.Q4_K_M.gguf\"\n","model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n","\n","streamstd = StreamingStdOutCallbackHandler()\n","tokencnt = CustomTokenCounter()\n","callback_manager = CallbackManager([streamstd, tokencnt])\n","\n","n_gpu_layers = 32  # Change this value based on your model and your GPU VRAM pool.\n","n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n","\n","llm = LlamaCpp(\n","    model_path=model_path,\n","    max_tokens=2500,\n","    n_gpu_layers=n_gpu_layers,\n","    n_batch=n_batch,\n","    callback_manager=callback_manager,\n","    n_ctx=2500, # Context window\n","    verbose=True, # Verbose is required to pass to the callback manager\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T12:15:07.328296Z","iopub.status.busy":"2023-10-11T12:15:07.327894Z","iopub.status.idle":"2023-10-11T12:17:30.795842Z","shell.execute_reply":"2023-10-11T12:17:30.794346Z","shell.execute_reply.started":"2023-10-11T12:15:07.328267Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[" The second law of thermodynamics states that entropy, or disorder, always increases over time in any isolated system, which means that energy can never be fully converted into another form without producing some waste heat or other forms of dissipation.  "]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time = 16644.78 ms\n","llama_print_timings:      sample time =    34.44 ms /    52 runs   (    0.66 ms per token,  1510.09 tokens per second)\n","llama_print_timings: prompt eval time = 16644.72 ms /    25 tokens (  665.79 ms per token,     1.50 tokens per second)\n","llama_print_timings:        eval time = 43265.92 ms /    51 runs   (  848.35 ms per token,     1.18 tokens per second)\n","llama_print_timings:       total time = 60177.49 ms\n","Llama.generate: prefix-match hit\n"]},{"name":"stdout","output_type":"stream","text":[" Albert Einstein was born on March 14, 1879."]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time = 16644.78 ms\n","llama_print_timings:      sample time =    11.81 ms /    18 runs   (    0.66 ms per token,  1524.39 tokens per second)\n","llama_print_timings: prompt eval time = 14568.79 ms /    22 tokens (  662.22 ms per token,     1.51 tokens per second)\n","llama_print_timings:        eval time = 14828.62 ms /    17 runs   (  872.27 ms per token,     1.15 tokens per second)\n","llama_print_timings:       total time = 29490.92 ms\n","Llama.generate: prefix-match hit\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","The Fermi Paradox refers to the apparent disconnect between our expectation of the likelihood of extraterrestrial life existing in the universe and the lack of empirical evidence or direct observation of such existence."]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time = 16644.78 ms\n","llama_print_timings:      sample time =    32.24 ms /    47 runs   (    0.69 ms per token,  1457.86 tokens per second)\n","llama_print_timings: prompt eval time = 12789.03 ms /    20 tokens (  639.45 ms per token,     1.56 tokens per second)\n","llama_print_timings:        eval time = 40482.68 ms /    46 runs   (  880.06 ms per token,     1.14 tokens per second)\n","llama_print_timings:       total time = 53518.63 ms\n"]}],"source":["chain = LLMChain(prompt=prompt, llm=llm)\n","\n","with get_openai_callback() as cb:\n","    chain.run({'question': \"Explain 2nd law of thermodynamics.\",})\n","    chain.run({'question': \"When was Einstein born? Give year, month day.\",})\n","    chain.run({'question': \"What is the Fermi paradox?\",})"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T12:17:30.802048Z","iopub.status.busy":"2023-10-11T12:17:30.800484Z","iopub.status.idle":"2023-10-11T12:17:30.812452Z","shell.execute_reply":"2023-10-11T12:17:30.810706Z","shell.execute_reply.started":"2023-10-11T12:17:30.801823Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\tTokens Used: 193\n","\tPrompt Tokens: 73\n","\tCompletion Tokens: 120\n","\n"]}],"source":["tokencnt.count(llm)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-10-11T12:17:30.815292Z","iopub.status.busy":"2023-10-11T12:17:30.814710Z","iopub.status.idle":"2023-10-11T12:17:30.830180Z","shell.execute_reply":"2023-10-11T12:17:30.828679Z","shell.execute_reply.started":"2023-10-11T12:17:30.815242Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[['Question: Explain 2nd law of thermodynamics.\\n\\nAnswer: Answer briefly in a sentence!'],\n"," ['Question: When was Einstein born? Give year, month day.\\n\\nAnswer: Answer briefly in a sentence!'],\n"," ['Question: What is the Fermi paradox?\\n\\nAnswer: Answer briefly in a sentence!']]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["tokencnt.llprompts"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
