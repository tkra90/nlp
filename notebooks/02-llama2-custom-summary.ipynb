{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Fine-tune Llama2 using QLoRA, Bits and Bytes and PEFT\n","***\n","\n","- **[QLoRA](https://arxiv.org/pdf/2305.14314.pdf): Quantized Low Rank Adapters** - a method for fine-tuning LLMs that uses a small number of quantized, updateable parameters to limit the complexity of training. This technique also allows those small sets of parameters to be added efficiently into the model itself, which means you can do fine-tuning on lots of data sets, potentially, and swap these \"adapters\" into your model when necessary.\n","\n","- **[Bits and Bytes](https://github.com/TimDettmers/bitsandbytes)**: Excellent package by Tim Dettmers et al., which provides a lightweight wrapper around custom CUDA functions that make LLMs go faster - optimizers, matrix mults and quantization.\n","\n","- **[PEFT](https://github.com/huggingface/peft): Parameter Efficient Fine-tuning**: Huggingface library that enables a number of PEFT methods, which again make it less expensive to fine-tune LLMs. These methods enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters"]},{"cell_type":"markdown","metadata":{},"source":["## Installs and imports"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T20:51:28.323770Z","iopub.status.busy":"2023-10-17T20:51:28.323268Z","iopub.status.idle":"2023-10-17T20:51:58.171286Z","shell.execute_reply":"2023-10-17T20:51:58.170371Z","shell.execute_reply.started":"2023-10-17T20:51:28.323743Z"},"trusted":true},"outputs":[],"source":["%pip install -qqq bitsandbytes==0.41.1\n","%pip install -qqq loralib==0.1.1\n","%pip install -qqq peft==0.5.0"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-10-17T20:51:58.173305Z","iopub.status.busy":"2023-10-17T20:51:58.173073Z","iopub.status.idle":"2023-10-17T20:52:09.863354Z","shell.execute_reply":"2023-10-17T20:52:09.862629Z","shell.execute_reply.started":"2023-10-17T20:51:58.173283Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import os\n","import re\n","import random\n","\n","from tqdm import tqdm\n","import seaborn as sns\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from wordcloud import WordCloud\n","\n","import transformers\n","from transformers import (\n","    AutoModelForCausalLM,      \n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments\n",")\n","\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","tqdm.pandas()\n","sns.set_style(\"dark\")\n","plt.rcParams[\"figure.figsize\"] = (20,8)\n","plt.rcParams[\"font.size\"] = 14"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T20:52:09.864679Z","iopub.status.busy":"2023-10-17T20:52:09.864408Z","iopub.status.idle":"2023-10-17T20:52:09.875856Z","shell.execute_reply":"2023-10-17T20:52:09.875096Z","shell.execute_reply.started":"2023-10-17T20:52:09.864657Z"},"trusted":true},"outputs":[{"data":{"text/plain":["42"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["def seed_everything(seed=None):\n","    \n","    max_seed_value = np.iinfo(np.uint32).max\n","    min_seed_value = np.iinfo(np.uint32).min\n","\n","    if (seed is None) or not (min_seed_value <= seed <= max_seed_value):\n","        seed = random.randint(np.iinfo(np.uint32).min, np.iinfo(np.uint32).max)\n","        \n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","    return seed\n","\n","seed_everything(42)"]},{"cell_type":"markdown","metadata":{},"source":["***\n","### Load model"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T20:52:09.877658Z","iopub.status.busy":"2023-10-17T20:52:09.877334Z","iopub.status.idle":"2023-10-17T20:55:34.257588Z","shell.execute_reply":"2023-10-17T20:55:34.256918Z","shell.execute_reply.started":"2023-10-17T20:52:09.877635Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"06dae9c878f84eac91236081f5791a0c","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Model size: 12852.51 MB\n","CPU times: user 6.54 s, sys: 9.67 s, total: 16.2 s\n","Wall time: 3min 23s\n"]}],"source":["%%time\n","\n","modelx = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\n","tokenizer = AutoTokenizer.from_pretrained(modelx)\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    modelx,\n","    torch_dtype=torch.float16,\n","    device_map='auto'\n",")\n","\n","# Calculate the size of the model in bytes\n","model_size_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n","\n","# Convert to megabytes (MB)\n","model_size_megabytes = model_size_bytes / (1024 * 1024)\n","\n","print(f\"Model size: {model_size_megabytes:.2f} MB\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T20:57:13.045167Z","iopub.status.busy":"2023-10-17T20:57:13.044328Z","iopub.status.idle":"2023-10-17T20:57:14.042097Z","shell.execute_reply":"2023-10-17T20:57:14.041142Z","shell.execute_reply.started":"2023-10-17T20:57:13.045130Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["13715\n"]}],"source":["!nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T20:57:26.795720Z","iopub.status.busy":"2023-10-17T20:57:26.795331Z","iopub.status.idle":"2023-10-17T20:57:27.299698Z","shell.execute_reply":"2023-10-17T20:57:27.298923Z","shell.execute_reply.started":"2023-10-17T20:57:26.795688Z"},"trusted":true},"outputs":[],"source":["del model\n","# del qmodel\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{},"source":["***\n","### Prepare and load model\n","Use Bits and Bytes with 4-bit quantization configuration, load the weights in 4-bit format, using a normal float 4 with double quantization to improve QLoRA's resolution. The weights are converted back to bfloat16 for weight updates."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T20:58:58.843517Z","iopub.status.busy":"2023-10-17T20:58:58.842915Z","iopub.status.idle":"2023-10-17T21:01:32.343388Z","shell.execute_reply":"2023-10-17T21:01:32.342701Z","shell.execute_reply.started":"2023-10-17T20:58:58.843490Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d73d3ab38d54242bf5ed82fb67785f2","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Model size: 3588.51 MB\n","CPU times: user 5.22 s, sys: 10.9 s, total: 16.1 s\n","Wall time: 2min 33s\n"]}],"source":["%%time\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_8bit=False,\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16,\n",")\n","\n","qmodel = AutoModelForCausalLM.from_pretrained(\n","    modelx,\n","    use_safetensors=True,\n","    quantization_config=bnb_config,\n","    trust_remote_code=True,\n","    device_map=\"auto\",\n",")\n","\n","# Calculate the size of the model in bytes\n","model_size_bytes = sum(p.numel() * p.element_size() for p in qmodel.parameters())\n","\n","# Convert to megabytes (MB)\n","model_size_megabytes = model_size_bytes / (1024 * 1024)\n","\n","print(f\"Model size: {model_size_megabytes:.2f} MB\")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-10-17T21:03:06.000728Z","iopub.status.busy":"2023-10-17T21:03:06.000088Z","iopub.status.idle":"2023-10-17T21:03:07.011416Z","shell.execute_reply":"2023-10-17T21:03:07.010500Z","shell.execute_reply.started":"2023-10-17T21:03:06.000696Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["5011\n"]}],"source":["!nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
