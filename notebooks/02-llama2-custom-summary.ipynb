{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Fine-tune Llama2 using QLoRA, Bits and Bytes and PEFT\n","***\n","\n","- **[QLoRA](https://arxiv.org/pdf/2305.14314.pdf): Quantized Low Rank Adapters** - a method for fine-tuning LLMs that uses a small number of quantized, updateable parameters to limit the complexity of training (simplifies weight storage). Instead of using precise float point 32 or 16 formats, weights are categorized into \"bins\" by their values. These bins can then be represented in more straightforward formats like int/float4 or int/float8 - a technique known as Quantization, often dubbed normal float 4 or 8. An added layer, double quantization, further refines these bins to conserve memory. Although the weights get quantized, during back-propagation, we revert them to float 16 to update. Post-update, these specific values are discarded.\n","\n","- **[Bits and Bytes](https://github.com/TimDettmers/bitsandbytes)**: Excellent package by Tim Dettmers et al., which provides a lightweight wrapper around custom CUDA functions that make LLMs go faster - optimizers, matrix mults and quantization.\n","\n","- **[PEFT](https://github.com/huggingface/peft): Parameter Efficient Fine-tuning**: Huggingface library that enables a number of PEFT methods, which again make it less expensive to fine-tune LLMs. These methods enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters"]},{"cell_type":"markdown","metadata":{},"source":["## Installs and imports"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-18T12:55:36.955114Z","iopub.status.busy":"2023-10-18T12:55:36.954581Z","iopub.status.idle":"2023-10-18T12:56:13.719016Z","shell.execute_reply":"2023-10-18T12:56:13.718021Z","shell.execute_reply.started":"2023-10-18T12:55:36.955088Z"},"trusted":true},"outputs":[],"source":["%pip install -qqq accelerate==0.22.0\n","%pip install -qqq bitsandbytes==0.41.1\n","%pip install -qqq loralib==0.1.2\n","%pip install -qqq peft==0.5.0\n"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-10-18T12:56:13.721102Z","iopub.status.busy":"2023-10-18T12:56:13.720647Z","iopub.status.idle":"2023-10-18T12:56:26.155995Z","shell.execute_reply":"2023-10-18T12:56:26.155391Z","shell.execute_reply.started":"2023-10-18T12:56:13.721076Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import os\n","import re\n","import random\n","\n","from tqdm import tqdm\n","import seaborn as sns\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from wordcloud import WordCloud\n","\n","import transformers\n","from transformers import (\n","    AutoModelForCausalLM,      \n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments\n",")\n","\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","\n","from peft import (\n","    LoraConfig,\n","    PeftConfig,\n","    PeftModel, \n","    get_peft_model,\n","    prepare_model_for_kbit_training\n",")\n","\n","import bitsandbytes as bnb\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","tqdm.pandas()\n","sns.set_style(\"dark\")\n","plt.rcParams[\"figure.figsize\"] = (20,8)\n","plt.rcParams[\"font.size\"] = 14"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-18T12:56:26.157282Z","iopub.status.busy":"2023-10-18T12:56:26.157058Z","iopub.status.idle":"2023-10-18T12:56:26.167446Z","shell.execute_reply":"2023-10-18T12:56:26.166653Z","shell.execute_reply.started":"2023-10-18T12:56:26.157263Z"},"trusted":true},"outputs":[{"data":{"text/plain":["42"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["def seed_everything(seed=None):\n","    \n","    max_seed_value = np.iinfo(np.uint32).max\n","    min_seed_value = np.iinfo(np.uint32).min\n","\n","    if (seed is None) or not (min_seed_value <= seed <= max_seed_value):\n","        seed = random.randint(np.iinfo(np.uint32).min, np.iinfo(np.uint32).max)\n","        \n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","    return seed\n","\n","seed_everything(42)"]},{"cell_type":"markdown","metadata":{},"source":["***\n","### Load original model\n","- Use Llama 2/7B-chat model for fine tuning and quantization"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-10-18T12:56:27.293471Z","iopub.status.busy":"2023-10-18T12:56:27.292954Z","iopub.status.idle":"2023-10-18T12:59:33.433829Z","shell.execute_reply":"2023-10-18T12:59:33.433123Z","shell.execute_reply.started":"2023-10-18T12:56:27.293419Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ec89fef48a2d4908ba20f091f690522b","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Model size: 12852.51 MB\n","CPU times: user 6.15 s, sys: 9.34 s, total: 15.5 s\n","Wall time: 3min 6s\n"]}],"source":["%%time\n","\n","modelx = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\n","tokenizer = AutoTokenizer.from_pretrained(modelx)\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    modelx,\n","    torch_dtype=torch.float16,\n","    device_map='auto'\n",")\n","\n","# Calculate the size of the model in bytes\n","model_size_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n","\n","# Convert to megabytes (MB)\n","model_size_megabytes = model_size_bytes / (1024 * 1024)\n","\n","print(f\"Model size: {model_size_megabytes:.2f} MB\")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-10-18T12:59:33.435156Z","iopub.status.busy":"2023-10-18T12:59:33.434940Z","iopub.status.idle":"2023-10-18T12:59:34.452565Z","shell.execute_reply":"2023-10-18T12:59:34.451689Z","shell.execute_reply.started":"2023-10-18T12:59:33.435137Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["13715\n"]}],"source":["!nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-10-18T13:00:54.661614Z","iopub.status.busy":"2023-10-18T13:00:54.660965Z","iopub.status.idle":"2023-10-18T13:00:55.176673Z","shell.execute_reply":"2023-10-18T13:00:55.175979Z","shell.execute_reply.started":"2023-10-18T13:00:54.661584Z"},"trusted":true},"outputs":[],"source":["del model\n","# del qmodel\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{},"source":["***\n","### Prepare model, set up BitsAndBytes config\n","- Use bitsandbytes with 4-bit quantization configuration\n","- reduce memory consumption considerably at a cost of some accuracy\n","- load the weights in 4-bit format, using a normal float 4 with double quantization to improve QLoRA's resolution\n","- weights are converted back to bfloat16 for weight updates.\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-10-18T13:01:11.019111Z","iopub.status.busy":"2023-10-18T13:01:11.018489Z","iopub.status.idle":"2023-10-18T13:04:21.099309Z","shell.execute_reply":"2023-10-18T13:04:21.098687Z","shell.execute_reply.started":"2023-10-18T13:01:11.019085Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cababf9f2fea47f59b32546ab9ca33e9","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Model size: 3588.51 MB\n","CPU times: user 4.82 s, sys: 11.1 s, total: 15.9 s\n","Wall time: 3min 10s\n"]}],"source":["%%time\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_8bit=False,\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16,\n",")\n","\n","qmodel = AutoModelForCausalLM.from_pretrained(\n","    modelx,\n","    use_safetensors=True,\n","    quantization_config=bnb_config,\n","    trust_remote_code=True,\n","    device_map=\"auto\",\n",")\n","\n","# Calculate the size of the model in bytes\n","model_size_bytes = sum(p.numel() * p.element_size() for p in qmodel.parameters())\n","\n","# Convert to megabytes (MB)\n","model_size_megabytes = model_size_bytes / (1024 * 1024)\n","\n","print(f\"Model size: {model_size_megabytes:.2f} MB\")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-10-18T13:05:14.043593Z","iopub.status.busy":"2023-10-18T13:05:14.042937Z","iopub.status.idle":"2023-10-18T13:05:15.061811Z","shell.execute_reply":"2023-10-18T13:05:15.060782Z","shell.execute_reply.started":"2023-10-18T13:05:14.043567Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["5011\n"]}],"source":["!nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits"]},{"cell_type":"markdown","metadata":{},"source":["### Preprocesses the model to ready it for training\n","- Load the tokenizer\n","- Enable Gradient Checkpointing: a technique that trades off computation time for memory. Instead of storing all intermediate activations in memory (as is typically done in the forward pass of back-propagation), gradient check-pointing stores only a subset of them. Then, during the backward pass, it recomputes the required activations on-the-fly. This way, the memory consumption is significantly reduced at the expense of additional computation."]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-10-18T13:50:23.351922Z","iopub.status.busy":"2023-10-18T13:50:23.351099Z","iopub.status.idle":"2023-10-18T13:50:23.463695Z","shell.execute_reply":"2023-10-18T13:50:23.462823Z","shell.execute_reply.started":"2023-10-18T13:50:23.351883Z"},"trusted":true},"outputs":[],"source":["tkn = AutoTokenizer.from_pretrained(modelx)\n","tkn.pad_token = tkn.eos_token\n","\n","qmodel.gradient_checkpointing_enable()\n","qmodel = prepare_model_for_kbit_training(qmodel)"]},{"cell_type":"markdown","metadata":{},"source":["### Sample question-answer"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-10-18T13:50:30.767763Z","iopub.status.busy":"2023-10-18T13:50:30.767201Z","iopub.status.idle":"2023-10-18T13:50:38.940118Z","shell.execute_reply":"2023-10-18T13:50:38.939365Z","shell.execute_reply.started":"2023-10-18T13:50:30.767735Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Result: Prompt: What is the theory of relativity? Explain in max 8 sentences. \n","The theory of relativity states that the laws of physics are the same for all observers in uniform motion relative to one another. This idea challenged the traditional concept of absolute time and space, as proposed by Isaac Newton. Einstein's theory of special relativity introduced the concept of time dilation, where time appears to pass slower for an observer in motion relative to a stationary observer. In addition, the theory of general relativity introduces the concept of gravity as a curvature of spacetime caused by the mass and energy of objects. Overall, the theory of relativity revolutionized our understanding of space and time, and has had significant implications for the development of modern physics.\n","CPU times: user 8.15 s, sys: 0 ns, total: 8.15 s\n","Wall time: 8.17 s\n"]}],"source":["%%time\n","\n","prompt_to_test = 'Prompt: What is the theory of relativity? Explain in max 8 sentences. \\n'\n","\n","pipeline = transformers.pipeline(\n","    \"text-generation\",\n","    model=qmodel,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n","    tokenizer = tkn,\n",")\n","\n","sequences = pipeline(\n","    prompt_to_test,\n","    do_sample=True,\n","    top_k=10,\n","    num_return_sequences=1,\n","    eos_token_id=tokenizer.eos_token_id,\n","    max_length=400,\n",")\n","for seq in sequences:\n","    print(f\"Result: {seq['generated_text']}\")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-10-18T13:38:51.091549Z","iopub.status.busy":"2023-10-18T13:38:51.090930Z","iopub.status.idle":"2023-10-18T13:38:51.099399Z","shell.execute_reply":"2023-10-18T13:38:51.098802Z","shell.execute_reply.started":"2023-10-18T13:38:51.091518Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 262410240 || all params: 3500412928 || trainable%: 7.496550989769399\n"]}],"source":["def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )\n","    \n","print_trainable_parameters(qmodel)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
