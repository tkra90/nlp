{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Fine-tune Llama2 using QLoRA, Bits and Bytes and PEFT\n","***\n","\n","- **[QLoRA](https://arxiv.org/pdf/2305.14314.pdf): Quantized Low Rank Adapters** - a method for fine-tuning LLMs that uses a small number of quantized, updateable parameters to limit the complexity of training (simplifies weight storage). Instead of using precise float point 32 or 16 formats, weights are categorized into \"bins\" by their values. These bins can then be represented in more straightforward formats like int/float4 or int/float8 - a technique known as Quantization, often dubbed normal float 4 or 8. An added layer, double quantization, further refines these bins to conserve memory. Although the weights get quantized, during back-propagation, we revert them to float 16 to update. Post-update, these specific values are discarded.\n","\n","- **[Bits and Bytes](https://github.com/TimDettmers/bitsandbytes)**: Excellent package by Tim Dettmers et al., which provides a lightweight wrapper around custom CUDA functions that make LLMs go faster - optimizers, matrix mults and quantization.\n","\n","- **[PEFT](https://github.com/huggingface/peft): Parameter Efficient Fine-tuning**: Huggingface library that enables a number of PEFT methods, which again make it less expensive to fine-tune LLMs. These methods enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters"]},{"cell_type":"markdown","metadata":{},"source":["## Installs and imports"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-19T18:16:37.534266Z","iopub.status.busy":"2023-10-19T18:16:37.534044Z","iopub.status.idle":"2023-10-19T18:17:18.596682Z","shell.execute_reply":"2023-10-19T18:17:18.595492Z","shell.execute_reply.started":"2023-10-19T18:16:37.534245Z"},"trusted":true},"outputs":[],"source":["%pip install -qqq accelerate==0.22.0\n","%pip install -qqq bitsandbytes==0.41.1\n","%pip install -qqq loralib==0.1.2\n","%pip install -qqq peft==0.5.0"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-10-19T18:17:18.598838Z","iopub.status.busy":"2023-10-19T18:17:18.598540Z","iopub.status.idle":"2023-10-19T18:17:31.490731Z","shell.execute_reply":"2023-10-19T18:17:31.489564Z","shell.execute_reply.started":"2023-10-19T18:17:18.598809Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import os\n","import re\n","import random\n","\n","from tqdm import tqdm\n","import seaborn as sns\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from wordcloud import WordCloud\n","\n","import transformers\n","from transformers import (\n","    AutoModelForCausalLM,      \n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments\n",")\n","\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","\n","from peft import (\n","    LoraConfig,\n","    PeftConfig,\n","    PeftModel, \n","    get_peft_model,\n","    prepare_model_for_kbit_training\n",")\n","\n","import bitsandbytes as bnb\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","tqdm.pandas()\n","sns.set_style(\"dark\")\n","plt.rcParams[\"figure.figsize\"] = (20,8)\n","plt.rcParams[\"font.size\"] = 14"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-10-19T18:17:31.492184Z","iopub.status.busy":"2023-10-19T18:17:31.491915Z","iopub.status.idle":"2023-10-19T18:17:31.503560Z","shell.execute_reply":"2023-10-19T18:17:31.502449Z","shell.execute_reply.started":"2023-10-19T18:17:31.492156Z"},"trusted":true},"outputs":[{"data":{"text/plain":["42"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["def seed_everything(seed=None):\n","    \n","    max_seed_value = np.iinfo(np.uint32).max\n","    min_seed_value = np.iinfo(np.uint32).min\n","\n","    if (seed is None) or not (min_seed_value <= seed <= max_seed_value):\n","        seed = random.randint(np.iinfo(np.uint32).min, np.iinfo(np.uint32).max)\n","        \n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","    return seed\n","\n","seed_everything(42)"]},{"cell_type":"markdown","metadata":{},"source":["***\n","### Load original model\n","- Use Llama 2/7B-chat model for fine tuning and quantization"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-10-19T18:17:32.587115Z","iopub.status.busy":"2023-10-19T18:17:32.586776Z","iopub.status.idle":"2023-10-19T18:20:21.381957Z","shell.execute_reply":"2023-10-19T18:20:21.381059Z","shell.execute_reply.started":"2023-10-19T18:17:32.587084Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d36c348c7da747bfbdfba8d0a6fc80a3","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Model size: 12852.51 MB\n","CPU times: user 7.18 s, sys: 9.11 s, total: 16.3 s\n","Wall time: 2min 48s\n"]}],"source":["%%time\n","\n","modelx = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\n","tokenizer = AutoTokenizer.from_pretrained(modelx)\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    modelx,\n","    torch_dtype=torch.float16,\n","    device_map='auto'\n",")\n","\n","# Calculate the size of the model in bytes\n","model_size_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n","\n","# Convert to megabytes (MB)\n","model_size_megabytes = model_size_bytes / (1024 * 1024)\n","\n","print(f\"Model size: {model_size_megabytes:.2f} MB\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-10-19T18:20:21.383400Z","iopub.status.busy":"2023-10-19T18:20:21.383103Z","iopub.status.idle":"2023-10-19T18:20:22.442790Z","shell.execute_reply":"2023-10-19T18:20:22.441807Z","shell.execute_reply.started":"2023-10-19T18:20:21.383365Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["13715\n"]}],"source":["!nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-10-19T18:20:22.445224Z","iopub.status.busy":"2023-10-19T18:20:22.444488Z","iopub.status.idle":"2023-10-19T18:20:22.947555Z","shell.execute_reply":"2023-10-19T18:20:22.946583Z","shell.execute_reply.started":"2023-10-19T18:20:22.445181Z"},"trusted":true},"outputs":[],"source":["del model\n","# del qmodel\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{},"source":["***\n","### Prepare model, set up BitsAndBytes config\n","- Use bitsandbytes with 4-bit quantization configuration\n","- reduce memory consumption considerably at a cost of some accuracy\n","- load the weights in 4-bit format, using a normal float 4 with double quantization to improve QLoRA's resolution\n","- weights are converted back to bfloat16 for weight updates.\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-10-19T18:20:22.948962Z","iopub.status.busy":"2023-10-19T18:20:22.948662Z","iopub.status.idle":"2023-10-19T18:20:30.166878Z","shell.execute_reply":"2023-10-19T18:20:30.166048Z","shell.execute_reply.started":"2023-10-19T18:20:22.948937Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2dc465ba0b64d028e6f9e74ac678543","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Model size: 3588.51 MB\n","CPU times: user 4.85 s, sys: 2.59 s, total: 7.44 s\n","Wall time: 7.21 s\n"]}],"source":["%%time\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_8bit=False,\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.float16,\n",")\n","\n","qmodel = AutoModelForCausalLM.from_pretrained(\n","    modelx,\n","    use_safetensors=True,\n","    quantization_config=bnb_config,\n","    trust_remote_code=True,\n","    device_map=\"auto\",\n",")\n","\n","# Calculate the size of the model in bytes\n","model_size_bytes = sum(p.numel() * p.element_size() for p in qmodel.parameters())\n","\n","# Convert to megabytes (MB)\n","model_size_megabytes = model_size_bytes / (1024 * 1024)\n","\n","print(f\"Model size: {model_size_megabytes:.2f} MB\")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-10-19T18:20:30.168249Z","iopub.status.busy":"2023-10-19T18:20:30.167959Z","iopub.status.idle":"2023-10-19T18:20:31.235910Z","shell.execute_reply":"2023-10-19T18:20:31.234713Z","shell.execute_reply.started":"2023-10-19T18:20:30.168225Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["5011\n"]}],"source":["!nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits"]},{"cell_type":"markdown","metadata":{},"source":["***\n","### Sample question-answer pipeline\n","- Load the tokenizer\n","- Run it before transforming model into 'PeftModelForCausalLM'"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-10-19T18:20:31.239561Z","iopub.status.busy":"2023-10-19T18:20:31.239179Z","iopub.status.idle":"2023-10-19T18:20:47.346272Z","shell.execute_reply":"2023-10-19T18:20:47.345379Z","shell.execute_reply.started":"2023-10-19T18:20:31.239518Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Result: Prompt: What is the theory of relativity? Explain in max 8 sentences. \n",">  answer: The theory of relativity, developed by Albert Einstein, is a fundamental concept in modern physics that challenges the traditional understanding of space and time. The theory posits that the laws of physics are the same for all observers in uniform motion relative to one another, and that time and space are relative rather than absolute. In other words, time can appear to slow down or speed up depending on an observer's motion and position, and the length of an object can increase or decrease depending on its speed relative to an observer. The theory of relativity has far-reaching implications, including the idea that the passage of time is relative, that time dilation occurs when objects move at high speeds, and that space and time are intertwined as a single entity called spacetime. The theory has been experimentally verified through numerous observations and experiments, and has led to a greater understanding of the nature of space and time in the universe.\n","CPU times: user 13.4 s, sys: 438 ms, total: 13.8 s\n","Wall time: 16.1 s\n"]}],"source":["%%time\n","\n","tkn = AutoTokenizer.from_pretrained(modelx)\n","tkn.pad_token = tkn.eos_token\n","\n","prompt_to_test = 'Prompt: What is the theory of relativity? Explain in max 8 sentences. \\n'\n","\n","pipeline = transformers.pipeline(\n","    \"text-generation\",\n","    model=qmodel,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n","    tokenizer = tkn,\n",")\n","\n","sequences = pipeline(\n","    prompt_to_test,\n","    do_sample=True,\n","    top_k=10,\n","    num_return_sequences=1,\n","    eos_token_id=tokenizer.eos_token_id,\n","    max_length=400,\n",")\n","for seq in sequences:\n","    print(f\"Result: {seq['generated_text']}\")"]},{"cell_type":"markdown","metadata":{},"source":["***\n","### Preprocesses the model to ready it for training\n","\n","- Enable Gradient Checkpointing: a technique that trades off computation time for memory. Instead of storing all intermediate activations in memory (as is typically done in the forward pass of back-propagation), gradient check-pointing stores only a subset of them. Then, during the backward pass, it recomputes the required activations on-the-fly. This way, the memory consumption is significantly reduced at the expense of additional computation.\n","- PEFT preparing a model before running a training. This includes:1- Cast the layernorm in fp32 2- making output embedding layer require grads 3- Add the upcasting of the lm head to fp32"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-10-19T18:20:47.347603Z","iopub.status.busy":"2023-10-19T18:20:47.347334Z","iopub.status.idle":"2023-10-19T18:20:47.358728Z","shell.execute_reply":"2023-10-19T18:20:47.357942Z","shell.execute_reply.started":"2023-10-19T18:20:47.347580Z"},"trusted":true},"outputs":[],"source":["qmodel.gradient_checkpointing_enable()\n","qmodel = prepare_model_for_kbit_training(qmodel)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-10-19T18:20:47.359977Z","iopub.status.busy":"2023-10-19T18:20:47.359711Z","iopub.status.idle":"2023-10-19T18:20:47.368725Z","shell.execute_reply":"2023-10-19T18:20:47.367845Z","shell.execute_reply.started":"2023-10-19T18:20:47.359956Z"},"trusted":true},"outputs":[],"source":["def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )\n","    \n","# print_trainable_parameters(qmodel)"]},{"cell_type":"markdown","metadata":{},"source":["***\n","### Set up the configuration for LORA\n","params: \n","- **r** - rank parameter,  which determines the rank of the adaptation matrix. Let's set r to 16. \n","- **lora_alpha** - scaling parameter, which modulates the scaling of learned weights. Higher alpha values yield fewer weight updates; specifically, weights are scaled by r/alpha. Let's opt for an alpha value of 32. \n","- **dropout rate** - set the dropout rate at 5% and designate the task as Causal Language Model\n","- **target_modules** is set using our helper functions - every layer identified by that function will be included in the PEFT update."]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-10-19T18:20:47.370347Z","iopub.status.busy":"2023-10-19T18:20:47.370077Z","iopub.status.idle":"2023-10-19T18:20:47.385832Z","shell.execute_reply":"2023-10-19T18:20:47.384997Z","shell.execute_reply.started":"2023-10-19T18:20:47.370325Z"},"trusted":true},"outputs":[],"source":["import re\n","def get_num_layers(model):\n","    numbers = set()\n","    for name, _ in model.named_parameters():\n","        for number in re.findall(r'\\d+', name):\n","            numbers.add(int(number))\n","    return max(numbers)\n","\n","def get_last_layer_linears(model):\n","    names = []\n","    \n","    num_layers = get_num_layers(model)\n","    for name, module in model.named_modules():\n","        if str(num_layers) in name and not \"encoder\" in name:\n","            if isinstance(module, torch.nn.Linear):\n","                names.append(name)\n","    return names"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-10-19T18:20:47.387191Z","iopub.status.busy":"2023-10-19T18:20:47.386938Z","iopub.status.idle":"2023-10-19T18:20:57.894129Z","shell.execute_reply":"2023-10-19T18:20:57.893126Z","shell.execute_reply.started":"2023-10-19T18:20:47.387170Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 8388608 || all params: 3508801536 || trainable%: 0.23907331075678143\n"]}],"source":["config = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    # target_modules=get_last_layer_linears(qmodel),\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","qmodel = get_peft_model(qmodel, config)\n","print_trainable_parameters(qmodel)\n","\n","# with get_last_layer:\n","# trainable params: 1249280 || all params: 3501662208 || trainable%: 0.03567677079604818"]},{"cell_type":"markdown","metadata":{},"source":["***\n","### Generation config\n","params:\n","- **top_p**: a method for choosing from among a selection of most probable outputs, as opposed to greedily just taking the highest (the model won't sample from the entire token distribution, instead, it will sample from a narrowed set of tokens whose combined probability surpasses a threshold\n","- **temperature**: a modulation on the softmax function used to determine the values of our outputs, lower val is more deterministic since values closer to 0 reduce randomness\n","- **num_return_sequences**: by setting it to 1, the model will produce only one output sequence (force the model to produce short answers)\n","- **max_new_tokens**: length of answer\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-10-19T18:20:57.896158Z","iopub.status.busy":"2023-10-19T18:20:57.895434Z","iopub.status.idle":"2023-10-19T18:20:57.901540Z","shell.execute_reply":"2023-10-19T18:20:57.900588Z","shell.execute_reply.started":"2023-10-19T18:20:57.896123Z"},"trusted":true},"outputs":[],"source":["generation_config = qmodel.generation_config\n","generation_config.max_new_tokens = 200\n","generation_config.temperature = 0.5\n","generation_config.top_p = 0.7\n","generation_config.num_return_sequences = 1\n","generation_config.pad_token_id = tkn.eos_token_id\n","generation_config.eos_token_id = tkn.eos_token_id"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-10-19T18:20:57.903041Z","iopub.status.busy":"2023-10-19T18:20:57.902802Z","iopub.status.idle":"2023-10-19T18:21:12.608682Z","shell.execute_reply":"2023-10-19T18:21:12.607668Z","shell.execute_reply.started":"2023-10-19T18:20:57.903021Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Prompt: What is the theory of relativity? Explain in max 8 sentences. \n","\n","The theory of relativity, developed by Albert Einstein, is a fundamental concept in modern physics that challenges our understanding of space and time. The theory posits that the laws of physics are the same for all observers in uniform motion relative to one another, and that the passage of time and the length of objects can vary depending on their speed and position in a gravitational field. The theory consists of two main parts: special relativity, which deals with objects moving at constant speeds relative to each other, and general relativity, which deals with gravity and its effects on spacetime. Special relativity introduces the concept of time dilation, where time appears to pass more slowly for an observer in motion relative to a stationary observer, and length contraction, where objects appear shorter to an observer in motion relative to them. General relativity introduces the concept of gravitational redshift, where light is affected by the curvature of spacetime caused by the presence\n","CPU times: user 14.7 s, sys: 0 ns, total: 14.7 s\n","Wall time: 14.7 s\n"]}],"source":["%%time\n","device = \"cuda\"\n","\n","encoding = tkn(prompt_to_test, return_tensors=\"pt\").to(device)\n","with torch.no_grad():\n","    outputs = qmodel.generate(\n","        input_ids = encoding.input_ids,\n","        attention_mask = encoding.attention_mask,\n","        generation_config = generation_config\n","    )\n","\n","print(tkn.decode(outputs[0], skip_special_tokens=True))"]},{"cell_type":"markdown","metadata":{},"source":["***\n","### Build Dataset"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
