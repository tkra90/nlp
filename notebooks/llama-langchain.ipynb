{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Introduction\n","The plan:\n","- load and use llama-cpp with langchain\n","- add custom CallbackHandler to track token usage"]},{"cell_type":"markdown","metadata":{},"source":["### Imports and installs"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-09-29T08:51:19.794560Z","iopub.status.busy":"2023-09-29T08:51:19.794108Z","iopub.status.idle":"2023-09-29T08:52:52.797467Z","shell.execute_reply":"2023-09-29T08:52:52.795657Z","shell.execute_reply.started":"2023-09-29T08:51:19.794520Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install -qqq langchain==0.0.304 --progress-bar off\n","%pip install -qqq llama-cpp-python==0.2.7 --progress-bar off"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-09-29T08:52:52.800861Z","iopub.status.busy":"2023-09-29T08:52:52.800427Z","iopub.status.idle":"2023-09-29T08:52:56.692815Z","shell.execute_reply":"2023-09-29T08:52:56.691573Z","shell.execute_reply.started":"2023-09-29T08:52:52.800823Z"},"trusted":true},"outputs":[],"source":["import os\n","import sys\n","import warnings\n","\n","warnings.simplefilter(\"ignore\")\n","\n","from huggingface_hub import hf_hub_download\n","from langchain.llms import LlamaCpp\n","from langchain import PromptTemplate, LLMChain\n","\n","from langchain.callbacks.manager import CallbackManager\n","from langchain.callbacks import get_openai_callback\n","from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler # handle std out of llm in jupyterNB"]},{"cell_type":"markdown","metadata":{},"source":["***\n","## Load Llama2-13b"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"execution":{"iopub.execute_input":"2023-09-29T08:52:56.694953Z","iopub.status.busy":"2023-09-29T08:52:56.694315Z","iopub.status.idle":"2023-09-29T09:00:47.823728Z","shell.execute_reply":"2023-09-29T09:00:47.822050Z","shell.execute_reply.started":"2023-09-29T08:52:56.694893Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGUF\"\n","model_basename = \"llama-2-13b-chat.Q4_K_M.gguf\"\n","model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n","\n","\n","callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n","\n","n_gpu_layers = 32  # Change this value based on your model and your GPU VRAM pool.\n","n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n","\n","\n","llm = LlamaCpp(\n","    model_path=model_path,\n","    max_tokens=2500,\n","    n_gpu_layers=n_gpu_layers,\n","    n_batch=n_batch,\n","    callback_manager=callback_manager,\n","    n_ctx=2500, # Context window\n","    verbose=True, # Verbose is required to pass to the callback manager\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Create prompt template and run chain"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-09-29T09:00:47.829013Z","iopub.status.busy":"2023-09-29T09:00:47.827137Z","iopub.status.idle":"2023-09-29T09:00:47.837035Z","shell.execute_reply":"2023-09-29T09:00:47.836017Z","shell.execute_reply.started":"2023-09-29T09:00:47.828947Z"},"trusted":true},"outputs":[],"source":["template = \"\"\"Question: {question}\n","\n","Answer: Answer briefly in a sentence!\"\"\"\n","\n","prompt = PromptTemplate(template=template, input_variables=[\"question\"])"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-09-29T09:00:47.840003Z","iopub.status.busy":"2023-09-29T09:00:47.839558Z","iopub.status.idle":"2023-09-29T09:01:37.573415Z","shell.execute_reply":"2023-09-29T09:01:37.572010Z","shell.execute_reply.started":"2023-09-29T09:00:47.839970Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Albert Einstein was born on March 14, 1879, in the town of Ulm, in the Kingdom of WÃ¼rttemberg, German Empire."]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time = 17844.41 ms\n","llama_print_timings:      sample time =    24.31 ms /    38 runs   (    0.64 ms per token,  1563.34 tokens per second)\n","llama_print_timings: prompt eval time = 17844.25 ms /    25 tokens (  713.77 ms per token,     1.40 tokens per second)\n","llama_print_timings:        eval time = 31662.92 ms /    37 runs   (  855.75 ms per token,     1.17 tokens per second)\n","llama_print_timings:       total time = 49710.74 ms\n"]}],"source":["chain = LLMChain(prompt=prompt, llm=llm)\n","\n","with get_openai_callback() as cb:\n","    result = chain.run({'question': \"When was Einstein born? Give year, month day.\",})"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
